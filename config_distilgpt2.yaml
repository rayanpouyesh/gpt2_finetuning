training:
  output_dir : './results'
  learning_rate: 1e-3
  epochs: 5
  gradient_steps : 2
  batch_size: 2
  save_steps: 1000
  logging_steps : 1
  save_strategy : "steps"
  evaluation_strategy : "no"
  save_total_limit : 2
#  eval_steps : 1000




model:
  model_name: "distil-gpt2"
  max_seq_length: 1024
